{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Quiz - Key concepts on Deep neural Networks\n",
    "\n",
    "##### 1. What is the \"cache\" used for in our implementation of forward propagation and backward propagation?\n",
    "\n",
    "> <input type=\"radio\" checked> We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.\n",
    "\n",
    "> <input type=\"radio\"> It is used to cache the intermediate values of the cost function during training.\n",
    "\n",
    "> <input type=\"radio\"> We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.\n",
    "\n",
    "> <input type=\"radio\"> It is used to keep track of the hyperparameters that we are searching over, to speed up computation.\n",
    "    \n",
    "##### 2. Among the following, which ones are \"hyperparameters\"? (Check all that apply.)\n",
    "\n",
    "> <input type=\"checkbox\"> bias vectors $ b^{[l]} $\n",
    "    \n",
    "> <input type=\"checkbox\" checked> learning rate $ \\alpha $\n",
    "    \n",
    "> <input type=\"checkbox\" checked> number of layers $ L $ in the neural network\n",
    "    \n",
    "> <input type=\"checkbox\" checked> size of the hidden layers $ n^{[l]} $\n",
    "\n",
    "> <input type=\"checkbox\"> activation values $ a^{[l]} $\n",
    "\n",
    "> <input type=\"checkbox\"> weight matrices $ W^{[l]} $\n",
    "\n",
    "> <input type=\"checkbox\" checked> number of iterations\n",
    "\n",
    "##### 3. Which of the following statements is true?\n",
    "\n",
    "> <input type=\"radio\" checked> The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.\n",
    "\n",
    "> <input type=\"radio\"> The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.\n",
    "\n",
    "##### 4. Vectorization allows you to compute forward propagation in an L-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, …,L. True/False?\n",
    "\n",
    "> <input type=\"radio\"> True\n",
    "\n",
    "> <input type=\"radio\" checked> False\n",
    "\n",
    "##### 5. Assume we store the values for $ n^{[l]} $ in an array called layers, as follows: layer_dims = [$ n_x $, 4,3,2,1]. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?\n",
    "\n",
    "> <input type=\"radio\"> Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(i in range(1, len(layer_dims)/2)):\n",
    "  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <input type=\"radio\"> Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(i in range(1, len(layer_dims)/2)):\n",
    "  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "  parameter[‘b’ + str(i)] = np.random.randn(layers[i-1], 1) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <input type=\"radio\"> Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(i in range(1, len(layer_dims))):\n",
    "  parameter[‘W’ + str(i)] = np.random.randn(layers[i-1], layers[i])) * 0.01\n",
    "  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <input type=\"radio\" checked> Code 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(i in range(1, len(layer_dims))):\n",
    "  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01\n",
    "  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Consider the following neural network. How many layers does this network have?\n",
    "\n",
    "<img src=img/04_01.png />\n",
    "\n",
    "> <input type=\"radio\" checked> The number of layers L is 4. The number of hidden layers is 3.\n",
    "\n",
    "> <input type=\"radio\"> The number of layers L is 3. The number of hidden layers is 3.\n",
    "\n",
    "> <input type=\"radio\"> The number of layers L is 4. The number of hidden layers is 4.\n",
    "\n",
    "> <input type=\"radio\"> The number of layers L is 5. The number of hidden layers is 4.\n",
    "\n",
    "##### 7. During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l, since the gradient depends on it. True/False?\n",
    "\n",
    "> <input type=\"radio\" checked> True\n",
    "\n",
    "> <input type=\"radio\"> False\n",
    "\n",
    "##### 8. There are certain functions with the following properties:\n",
    "\n",
    "(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?\n",
    "\n",
    "> <input type=\"radio\" checked> True\n",
    "\n",
    "> <input type=\"radio\"> False\n",
    "\n",
    "##### 9. Consider the following 2 hidden layer neural network. Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "<img src = img/04_02.png />\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ W^{[1]} $ will have shape (4, 4)\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ b^{[1]} $ will have shape (4, 1)\n",
    "\n",
    "> <input type=\"checkbox\"> $ W^{[1]} $ will have shape (3, 4)\n",
    "\n",
    "> <input type=\"checkbox\"> $ b^{[1]} $ will have shape (3, 1)\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ W^{[2]} $ will have shape (3, 4)\n",
    "\n",
    "> <input type=\"checkbox\"> $ b^{[2]} $ will have shape (1, 1)\n",
    "\n",
    "> <input type=\"checkbox\"> $ W^{[2]} $ will have shape (3, 1)\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ b^{[2]} $ will have shape (3, 1)\n",
    "\n",
    "> <input type=\"checkbox\"> $ W^{[3]} $ will have shape (3, 1)\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ b^{[3]} $ will have shape (1, 1)\n",
    "\n",
    "> <input type=\"checkbox\" checked> $ W^{[3]} $ will have shape (1, 3)\n",
    "\n",
    "> <input type=\"checkbox\"> $ b^{[3]} $ will have shape (3, 1)\n",
    "\n",
    "##### 10. Whereas the previous question used a specific network, in the general case what is the dimension of $ W^{[l]} $, the weight matrix associated with layer l?\n",
    "\n",
    "> <input type=\"radio\"> $ W^{[l]} $ has shape $ (n^{[l-1]}, n^{[l]}) $\n",
    "\n",
    "> <input type=\"radio\" checked> $ W^{[l]} $ has shape $ (n^{[l]}, n^{[l-1]}) $\n",
    "\n",
    "> <input type=\"radio\"> $ W^{[l]} $ has shape $ (n^{[l+1]}, n^{[l]}) $\n",
    "\n",
    "> <input type=\"radio\"> $ W^{[l]} $ has shape $ (n^{[l]}, n^{[l+1]}) $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
